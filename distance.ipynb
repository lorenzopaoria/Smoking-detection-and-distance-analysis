{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzopaoria/Smoking-detection-and-distance-analysis/blob/main/distance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIJA1DQVax5A"
      },
      "source": [
        "Find the distance between smoker and not with help of depth model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4xwcHdcZuvU",
        "outputId": "54b239fb-4278-4241-ad88-970652fd015e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Cloning into 'Depth-Anything-V2'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 142 (delta 45), reused 34 (delta 34), pack-reused 67 (from 2)\u001b[K\n",
            "Receiving objects: 100% (142/142), 45.17 MiB | 45.26 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n",
            "/content/Depth-Anything-V2/Depth-Anything-V2\n",
            "Requirement already satisfied: gradio_imageslider in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (0.0.20)\n",
            "Requirement already satisfied: gradio==4.29.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.29.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.11.0.86)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.20.1+cu124)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==0.16.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.16.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (6.5.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.9.9)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.29.0->-r requirements.txt (line 2)) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 2)) (2024.10.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 2)) (11.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (1.28.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (13.9.4)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->gradio==4.29.0->-r requirements.txt (line 2)) (0.46.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 2)) (0.23.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.24.1->gradio==4.29.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.29.0->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 2)) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Installazione delle dipendenze\n",
        "!pip install torch torchvision\n",
        "!pip install opencv-python\n",
        "!git clone https://github.com/DepthAnything/Depth-Anything-V2.git\n",
        "%cd Depth-Anything-V2\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "90c--lc6ax5H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGhWYCnQax5K",
        "outputId": "8ce58c20-96ff-48f9-a4ec-ab409269dce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLTVRQ6nXJPt",
        "outputId": "a737289d-8f8f-4390-f9c2-b3c468bfda8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo in uso: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-e5f78b13b715>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello vits caricato con successo su cuda\n"
          ]
        }
      ],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "# configurazione del dispositivo (GPU T4 su Colab)\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Dispositivo in uso: {DEVICE}\")\n",
        "\n",
        "# configurazione del modello DepthAnythingV2\n",
        "model_configs = {\n",
        "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]}\n",
        "}\n",
        "\n",
        "encoder = 'vits'\n",
        "model = DepthAnythingV2(**model_configs[encoder])\n",
        "\n",
        "# caricamento dei pesi del modello\n",
        "checkpoint_path = f'/content/drive/MyDrive/pth_depth_estimation_large/depth_anything_v2_{encoder}.pth'\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
        "model = model.to(DEVICE).eval()\n",
        "print(f\"Modello {encoder} caricato con successo su {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vJYauE2pbOJu"
      },
      "outputs": [],
      "source": [
        "# Definizione della classe Person\n",
        "@dataclass\n",
        "class Person:\n",
        "    x1: int\n",
        "    y1: int\n",
        "    x2: int\n",
        "    y2: int\n",
        "    is_smoking: bool\n",
        "    confidence: float\n",
        "    class_id: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1wpy_24bjOzH"
      },
      "outputs": [],
      "source": [
        "# Funzione per calcolare il punto centrale di una bounding box\n",
        "def calculate_center_point(person: Person) -> Tuple[float, float]:\n",
        "    center_x = (person.x1 + person.x2) / 2\n",
        "    center_y = (person.y1 + person.y2) / 2\n",
        "    return (center_x, center_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mn-BQZSIjTXo"
      },
      "outputs": [],
      "source": [
        "# Funzione per ridimensionare l'immagine a un multiplo della patch\n",
        "def resize_to_multiple_of_patch(image, patch_size=14):\n",
        "    h, w = image.shape[:2]\n",
        "    new_h = math.ceil(h / patch_size) * patch_size\n",
        "    new_w = math.ceil(w / patch_size) * patch_size\n",
        "    return cv2.resize(image, (new_w, new_h))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VBPSx0lAatBb"
      },
      "outputs": [],
      "source": [
        "# Funzione per preprocessare l'immagine per la depth estimation\n",
        "def preprocess_image_for_depth(image):\n",
        "    resized_image = resize_to_multiple_of_patch(image)\n",
        "    image_tensor = torch.from_numpy(resized_image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    return resized_image, image_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m8ESvqphax5Q"
      },
      "outputs": [],
      "source": [
        "# Funzione per calcolare la mappa di profondità\n",
        "def calculate_depth_map(image_tensor, model):\n",
        "    with torch.no_grad():\n",
        "        depth_map = model(image_tensor.to(DEVICE))\n",
        "    return depth_map.squeeze().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DZIUMc2Aax5R"
      },
      "outputs": [],
      "source": [
        "# Funzione per scalare le bounding box in base al ridimensionamento dell'immagine\n",
        "def scale_bounding_boxes(people, original_image, resized_image):\n",
        "    orig_h, orig_w = original_image.shape[:2]\n",
        "    new_h, new_w = resized_image.shape[:2]\n",
        "\n",
        "    scale_x = new_w / orig_w\n",
        "    scale_y = new_h / orig_h\n",
        "\n",
        "    scaled_people = []\n",
        "    for person in people:\n",
        "        scaled_person = Person(\n",
        "            x1=int(person.x1 * scale_x),\n",
        "            y1=int(person.y1 * scale_y),\n",
        "            x2=int(person.x2 * scale_x),\n",
        "            y2=int(person.y2 * scale_y),\n",
        "            is_smoking=person.is_smoking,\n",
        "            confidence=person.confidence,\n",
        "            class_id=person.class_id\n",
        "        )\n",
        "        scaled_people.append(scaled_person)\n",
        "\n",
        "    return scaled_people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3J7n2BjYqUr9"
      },
      "outputs": [],
      "source": [
        "# Funzione per calcolare la distanza 3D tra due persone\n",
        "def calculate_3d_distance(p1: Person, p2: Person, depth_map, focal_length: float, avg_person_height: float = 1.70):\n",
        "    \"\"\"\n",
        "    Calcola la distanza 3D tra due persone utilizzando la mappa di profondità,\n",
        "    una stima della focale e l'altezza media di una persona.\n",
        "\n",
        "    Args:\n",
        "        p1, p2: Oggetti Person\n",
        "        depth_map: Mappa di profondità\n",
        "        focal_length: Lunghezza focale in pixel\n",
        "        avg_person_height: Altezza media di una persona in metri (default 1.70m)\n",
        "\n",
        "    Returns:\n",
        "        Distanza in metri\n",
        "    \"\"\"\n",
        "    # Calcola i punti centrali delle bounding box\n",
        "    c1 = calculate_center_point(p1)\n",
        "    c2 = calculate_center_point(p2)\n",
        "\n",
        "    # Assicurati che i punti centrali siano all'interno della mappa di profondità\n",
        "    c1_x, c1_y = max(0, min(int(c1[0]), depth_map.shape[1]-1)), max(0, min(int(c1[1]), depth_map.shape[0]-1))\n",
        "    c2_x, c2_y = max(0, min(int(c2[0]), depth_map.shape[1]-1)), max(0, min(int(c2[1]), depth_map.shape[0]-1))\n",
        "\n",
        "    # Utilizza un'area per migliorare la stima della profondità\n",
        "    radius = 5\n",
        "    p1_area = depth_map[max(0, c1_y-radius):min(depth_map.shape[0], c1_y+radius),\n",
        "                        max(0, c1_x-radius):min(depth_map.shape[1], c1_x+radius)]\n",
        "    p2_area = depth_map[max(0, c2_y-radius):min(depth_map.shape[0], c2_y+radius),\n",
        "                        max(0, c2_x-radius):min(depth_map.shape[1], c2_x+radius)]\n",
        "\n",
        "    # Usa la mediana per ridurre l'effetto di outlier\n",
        "    depth1 = np.median(p1_area) if p1_area.size > 0 else depth_map[c1_y, c1_x]\n",
        "    depth2 = np.median(p2_area) if p2_area.size > 0 else depth_map[c2_y, c2_x]\n",
        "\n",
        "    # Calcola la scala dei pixel in base all'altezza media di una persona\n",
        "    # Supponiamo che l'altezza della bounding box sia proporzionale all'altezza reale della persona\n",
        "    p1_height_pixels = p1.y2 - p1.y1\n",
        "    p2_height_pixels = p2.y2 - p2.y1\n",
        "\n",
        "    # Calcola la scala dei pixel in metri per ogni persona\n",
        "    scale1 = avg_person_height / p1_height_pixels\n",
        "    scale2 = avg_person_height / p2_height_pixels\n",
        "\n",
        "    # Converte i valori di profondità in metri\n",
        "    depth1_meters = depth1 * scale1\n",
        "    depth2_meters = depth2 * scale2\n",
        "\n",
        "    # Calcola le coordinate spaziali 3D\n",
        "    x1 = (c1_x - depth_map.shape[1] / 2) * depth1_meters / focal_length\n",
        "    y1 = (c1_y - depth_map.shape[0] / 2) * depth1_meters / focal_length\n",
        "    z1 = depth1_meters\n",
        "\n",
        "    x2 = (c2_x - depth_map.shape[1] / 2) * depth2_meters / focal_length\n",
        "    y2 = (c2_y - depth_map.shape[0] / 2) * depth2_meters / focal_length\n",
        "    z2 = depth2_meters\n",
        "\n",
        "    # Calcola la distanza euclidea 3D\n",
        "    distance_3d = math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n",
        "\n",
        "    return distance_3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5MbsTMTLbfC0"
      },
      "outputs": [],
      "source": [
        "# Funzione per trovare le distanze tra fumatori e non fumatori\n",
        "def find_smoker_nonsmoker_distances(people: List[Person], depth_map, focal_length: float, avg_person_height: float = 1.70) -> List[Dict]:\n",
        "    smokers = [p for p in people if p.class_id == 2]  # classe 2 per i fumatori\n",
        "    non_smokers = [p for p in people if p.class_id == 1]  # classe 1 per i non fumatori\n",
        "    distances_data = []\n",
        "\n",
        "    for i, smoker in enumerate(smokers):\n",
        "        for j, non_smoker in enumerate(non_smokers):\n",
        "            distance = calculate_3d_distance(smoker, non_smoker, depth_map, focal_length, avg_person_height)\n",
        "\n",
        "            distance_info = {\n",
        "                \"smoker_id\": i,\n",
        "                \"smoker_confidence\": smoker.confidence,\n",
        "                \"smoker_bbox\": [smoker.x1, smoker.y1, smoker.x2, smoker.y2],\n",
        "                \"non_smoker_id\": j,\n",
        "                \"non_smoker_confidence\": non_smoker.confidence,\n",
        "                \"non_smoker_bbox\": [non_smoker.x1, non_smoker.y1, non_smoker.x2, non_smoker.y2],\n",
        "                \"distance_meters\": float(distance)\n",
        "            }\n",
        "            distances_data.append(distance_info)\n",
        "\n",
        "    return distances_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vjzlPWVxkLUs"
      },
      "outputs": [],
      "source": [
        "# Funzione per caricare le detection da un file JSON\n",
        "def load_detections_from_json(json_path: str) -> List[Person]:\n",
        "    \"\"\"Carica le detection dal file JSON e le converte in oggetti Person.\"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    people = []\n",
        "    if 'detections' in data:\n",
        "        for detection in data['detections']:\n",
        "            class_id = detection.get('class')\n",
        "            # Carica solo le classi 1 (non fumatori) e 2 (fumatori)\n",
        "            if class_id in [1, 2]:\n",
        "                is_smoking = class_id == 2\n",
        "                bbox = detection.get('bbox', [0, 0, 0, 0])\n",
        "                confidence = detection.get('confidence', 0.0)\n",
        "\n",
        "                if len(bbox) == 4:\n",
        "                    people.append(Person(\n",
        "                        x1=int(bbox[0]),\n",
        "                        y1=int(bbox[1]),\n",
        "                        x2=int(bbox[2]),\n",
        "                        y2=int(bbox[3]),\n",
        "                        is_smoking=is_smoking,\n",
        "                        confidence=confidence,\n",
        "                        class_id=class_id\n",
        "                    ))\n",
        "\n",
        "    return people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3vfyIteXax5S"
      },
      "outputs": [],
      "source": [
        "# Funzione per processare e salvare l'immagine con le distanze\n",
        "def process_and_save_image(image_path: str, people: List[Person], output_dir: str, focal_length: float, avg_person_height: float, model) -> bool:\n",
        "    \"\"\"Processa un'immagine disegnando le distanze 3D tra i centri delle bounding box.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    original_image = cv2.imread(image_path)\n",
        "    if original_image is None:\n",
        "        print(f\"Errore nel caricamento dell'immagine: {image_path}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        resized_image, image_tensor = preprocess_image_for_depth(original_image)\n",
        "        scaled_people = scale_bounding_boxes(people, original_image, resized_image)\n",
        "        depth_map = calculate_depth_map(image_tensor, model)\n",
        "        depth_display = cv2.resize(depth_map, (original_image.shape[1], original_image.shape[0]))\n",
        "        normalized_depth = cv2.normalize(depth_display, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "        colored_depth = cv2.applyColorMap(normalized_depth, cv2.COLORMAP_INFERNO)\n",
        "\n",
        "        distances_data = find_smoker_nonsmoker_distances(scaled_people, depth_map, focal_length, avg_person_height)\n",
        "\n",
        "        # Salva i dati delle distanze in un file JSON\n",
        "        filename = os.path.basename(image_path)\n",
        "        json_output_path = os.path.join(output_dir, f\"distances_{os.path.splitext(filename)[0]}.json\")\n",
        "        with open(json_output_path, 'w') as f:\n",
        "            json.dump({\n",
        "                \"image_name\": filename,\n",
        "                \"focal_length_pixel\": focal_length,\n",
        "                \"avg_person_height_meters\": avg_person_height,\n",
        "                \"distances\": distances_data\n",
        "            }, f, indent=4)\n",
        "\n",
        "        visualized_image = resized_image.copy()\n",
        "\n",
        "        RED = (0, 0, 255)      # Fumatori\n",
        "        BLUE = (255, 0, 0)     # Non fumatori\n",
        "        YELLOW = (0, 255, 255) # Punti centrali\n",
        "        BROWN = (42, 42, 165)  # Linee\n",
        "        GREEN = (0, 255, 0)    # Testo delle distanze\n",
        "\n",
        "        # Disegna le bounding box per le classi 1 e 2\n",
        "        for person in scaled_people:\n",
        "            if person.class_id in [1, 2]:\n",
        "                color = RED if person.class_id == 2 else BLUE\n",
        "                cv2.rectangle(visualized_image, (int(person.x1), int(person.y1)),\n",
        "                            (int(person.x2), int(person.y2)), color, 2)\n",
        "                center = calculate_center_point(person)\n",
        "                cv2.circle(visualized_image, (int(center[0]), int(center[1])), 5, YELLOW, -1)\n",
        "\n",
        "        # Disegna le linee e le distanze\n",
        "        smokers = [p for p in scaled_people if p.class_id == 2]\n",
        "        non_smokers = [p for p in scaled_people if p.class_id == 1]\n",
        "\n",
        "        # Offset per separare le linee\n",
        "        offset_increment = 15\n",
        "        current_offset = 0\n",
        "\n",
        "        for i, smoker in enumerate(smokers):\n",
        "            for j, non_smoker in enumerate(non_smokers):\n",
        "                s_center = calculate_center_point(smoker)\n",
        "                ns_center = calculate_center_point(non_smoker)\n",
        "                distance = calculate_3d_distance(smoker, non_smoker, depth_map, focal_length, avg_person_height)\n",
        "\n",
        "                offset_y = current_offset\n",
        "                if i % 2 == 0:\n",
        "                    offset_y = -offset_y\n",
        "\n",
        "                mid_x = (s_center[0] + ns_center[0]) // 2\n",
        "                mid_y = (s_center[1] + ns_center[1]) // 2 + offset_y\n",
        "\n",
        "                cv2.line(visualized_image,\n",
        "                        (int(s_center[0]), int(s_center[1])),\n",
        "                        (int(mid_x), int(mid_y)),\n",
        "                        BROWN, 2)\n",
        "                cv2.line(visualized_image,\n",
        "                        (int(mid_x), int(mid_y)),\n",
        "                        (int(ns_center[0]), int(ns_center[1])),\n",
        "                        BROWN, 2)\n",
        "\n",
        "                text = f\"S{i}-NS{j}: {distance:.2f}m\"\n",
        "                (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "\n",
        "                cv2.rectangle(visualized_image,\n",
        "                            (int(mid_x - text_w/2 - 5), int(mid_y - text_h - 5)),\n",
        "                            (int(mid_x + text_w/2 + 5), int(mid_y + 5)),\n",
        "                            (255, 255, 255), -1)\n",
        "\n",
        "                cv2.putText(visualized_image, text,\n",
        "                          (int(mid_x - text_w/2), int(mid_y)),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, GREEN, 2)\n",
        "\n",
        "                current_offset += offset_increment\n",
        "\n",
        "                if current_offset > 80:\n",
        "                    current_offset = 15\n",
        "\n",
        "        filename = os.path.basename(image_path)\n",
        "        output_path = os.path.join(output_dir, f\"distances_{filename}\")\n",
        "        cv2.imwrite(output_path, visualized_image)\n",
        "\n",
        "        depth_output_path = os.path.join(output_dir, f\"depth_{filename}\")\n",
        "        cv2.imwrite(depth_output_path, colored_depth)\n",
        "\n",
        "        h, w = original_image.shape[:2]\n",
        "        resized_original = cv2.resize(original_image, (w, h))\n",
        "        resized_depth = cv2.resize(colored_depth, (w, h))\n",
        "        resized_visualization = cv2.resize(visualized_image, (w, h))\n",
        "\n",
        "        composite = np.hstack((resized_original, resized_depth, resized_visualization))\n",
        "        composite_output_path = os.path.join(output_dir, f\"composite_{filename}\")\n",
        "        cv2.imwrite(composite_output_path, composite)\n",
        "\n",
        "        print(f\"Elaborazione completata per {image_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante l'elaborazione di {image_path}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "haHiYRs6dLz-",
        "outputId": "131eabfe-24ca-4eac-b593-77532613aaf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione delle immagini in /content/drive/MyDrive/trained_photos/images...\n",
            "Trovate 73 immagini da elaborare.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   1%|▏         | 1/73 [00:04<05:46,  4.81s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_1.jpg\n",
            "Immagine trained_1.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   3%|▎         | 2/73 [00:09<05:22,  4.55s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_2.jpg\n",
            "Immagine trained_2.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   4%|▍         | 3/73 [00:12<04:51,  4.16s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_3.jpg\n",
            "Immagine trained_3.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   5%|▌         | 4/73 [00:16<04:37,  4.03s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_4.jpg\n",
            "Immagine trained_4.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   7%|▋         | 5/73 [00:20<04:33,  4.02s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_5.jpg\n",
            "Immagine trained_5.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:   8%|▊         | 6/73 [00:24<04:22,  3.92s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_6.jpg\n",
            "Immagine trained_6.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  10%|▉         | 7/73 [00:28<04:18,  3.92s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_7.jpg\n",
            "Immagine trained_7.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  11%|█         | 8/73 [00:32<04:21,  4.03s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_8.jpg\n",
            "Immagine trained_8.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  12%|█▏        | 9/73 [00:36<04:13,  3.97s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_9.jpg\n",
            "Immagine trained_9.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  14%|█▎        | 10/73 [00:40<04:03,  3.86s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_10.jpg\n",
            "Immagine trained_10.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  15%|█▌        | 11/73 [00:43<03:56,  3.81s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_11.jpg\n",
            "Immagine trained_11.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  16%|█▋        | 12/73 [00:48<04:00,  3.94s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_12.jpg\n",
            "Immagine trained_12.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  18%|█▊        | 13/73 [00:51<03:51,  3.86s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_13.jpg\n",
            "Immagine trained_13.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Elaborazione immagini:  19%|█▉        | 14/73 [00:55<03:50,  3.91s/img]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaborazione completata per /content/drive/MyDrive/trained_photos/images/trained_14.jpg\n",
            "Immagine trained_14.jpg elaborata con successo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rElaborazione immagini:  19%|█▉        | 14/73 [00:58<04:06,  4.17s/img]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-2fe6ca2a1bc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2fe6ca2a1bc1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_and_save_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_person_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0msuccessful\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-4eb8ea782e94>\u001b[0m in \u001b[0;36mprocess_and_save_image\u001b[0;34m(image_path, people, output_dir, focal_length, avg_person_height, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mresized_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image_for_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mscaled_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresized_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdepth_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_depth_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdepth_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnormalized_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORM_MINMAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCV_8U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-db0478c1af53>\u001b[0m in \u001b[0;36mcalculate_depth_map\u001b[0;34m(image_tensor, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdepth_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdepth_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    base_dir = '/content/drive/MyDrive/trained_photos'\n",
        "    output_dir = '/content/drive/MyDrive/distance_img_process'\n",
        "\n",
        "    # Parametri stimati\n",
        "    focal_length = 1000  # Lunghezza focale in pixel\n",
        "    avg_person_height = 1.70  # Altezza media di una persona in metri\n",
        "\n",
        "    images_dir = os.path.join(base_dir, 'images')\n",
        "    coordinates_dir = os.path.join(base_dir, 'coordinates')\n",
        "\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "    skipped = 0\n",
        "\n",
        "    print(f\"Elaborazione delle immagini in {images_dir}...\")\n",
        "\n",
        "    if not os.path.exists(images_dir):\n",
        "        print(f\"La directory delle immagini {images_dir} non esiste!\")\n",
        "        return\n",
        "    if not os.path.exists(coordinates_dir):\n",
        "        print(f\"La directory delle coordinate {coordinates_dir} non esiste!\")\n",
        "        return\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    print(f\"Trovate {len(image_files)} immagini da elaborare.\")\n",
        "\n",
        "    # Utilizzo di tqdm per la barra di progresso\n",
        "    for i, filename in enumerate(tqdm(image_files, desc=\"Elaborazione immagini\", unit=\"img\")):\n",
        "        # Commento precedente: print(f\"[{i+1}/{len(image_files)}] Elaborazione di {filename}...\")\n",
        "\n",
        "        image_path = os.path.join(images_dir, filename)\n",
        "        json_name = f\"{os.path.splitext(filename)[0]}.json\"\n",
        "        json_path = os.path.join(coordinates_dir, json_name)\n",
        "\n",
        "        if not os.path.exists(json_path):\n",
        "            tqdm.write(f\"File JSON non trovato per {filename}, saltato.\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            people = load_detections_from_json(json_path)\n",
        "            if not people:\n",
        "                tqdm.write(f\"Nessuna persona rilevata in {filename}, saltato.\")\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            success = process_and_save_image(image_path, people, output_dir, focal_length, avg_person_height, model)\n",
        "            if success:\n",
        "                successful += 1\n",
        "                tqdm.write(f\"Immagine {filename} elaborata con successo\")\n",
        "            else:\n",
        "                failed += 1\n",
        "                tqdm.write(f\"Errore nell'elaborazione dell'immagine {filename}\")\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Errore catastrofico nell'elaborazione di {filename}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            failed += 1\n",
        "\n",
        "    print(f\"\\n=== RIEPILOGO DELL'ELABORAZIONE ===\")\n",
        "    print(f\"Immagini elaborate con successo: {successful}\")\n",
        "    print(f\"Immagini non elaborate (errori): {failed}\")\n",
        "    print(f\"Immagini saltate (file mancanti o nessuna persona): {skipped}\")\n",
        "    print(f\"Totale immagini processate: {successful + failed} di {len(image_files)}\")\n",
        "    print(f\"Risultati salvati in: {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}